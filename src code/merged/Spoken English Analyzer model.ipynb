{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS] hi my name is akshay i am from bangalore and i am studying masters from computer science in triple it bangalore [SEP]': 'Wrong', '[CLS] and electronic circuits from very passionate and [SEP]': 'Wrong'}\n",
      "Vocabulary strength:  141.6846511627907\n",
      "Unique words spoken:  16\n",
      "hi my name is akshay i am from bangalore and i am studying masters from computer science in triple it bangaloreand electronic circuits from very passionate and\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertForSequenceClassification\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "seconds = 20\n",
    "minute = 1\n",
    "t_end = time.time() + (seconds * minute)  ## Takes voice input for this time-frame\n",
    "r = sr.Recognizer()\n",
    "\n",
    "sentences = []\n",
    "inp_corpus = \"\"\n",
    "while time.time() < t_end:\n",
    "    try:\n",
    "        with sr.Microphone() as source2:\n",
    "            r.adjust_for_ambient_noise(source2, duration=1)\n",
    "            audio2 = r.listen(source2)\n",
    "            MyText = r.recognize_google(audio2)\n",
    "            MyText = MyText.lower()\n",
    "            inp_corpus += MyText\n",
    "            sentences.append(MyText)\n",
    "\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results; {0}\".format(e))\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"unknown error occured\")\n",
    "        \n",
    "\n",
    "## Pre-processing\n",
    "bad_chars = [';', ':', '!', \"*\", \",\", \".\"]\n",
    "\n",
    "inp_corpus = ''.join(i for i in inp_corpus if not i in bad_chars)\n",
    "inp_corpus = inp_corpus.replace('&', 'and')\n",
    "inp_corpus = inp_corpus.replace('@', 'at')\n",
    "inp_corpus = inp_corpus.lower()\n",
    "\n",
    "corpus_list = list(inp_corpus.split(' '))\n",
    "\n",
    "## setting weightage parameters for scores\n",
    "alpha = 0.4\n",
    "beta = 0.6\n",
    "\n",
    "## textfile contatining words and their usage frequency\n",
    "f = open(\"dependencies/dict_10k.txt\", \"r\")\n",
    "\n",
    "dictonary = dict()\n",
    "\n",
    "list_lines = f.readlines()\n",
    "\n",
    "## creating word-frequency dictonary out of words-text file\n",
    "rank = 1\n",
    "for line in list_lines:\n",
    "    li = list(line.split(' '))\n",
    "    word, frequency = li[0], li[2]\n",
    "    dictonary[word] = rank\n",
    "    rank += 1\n",
    "\n",
    "## scaling the ranks to (1-100) range\n",
    "def scale_100(no):\n",
    "    scaled = (no - 1) * 99 / (len(dictonary) - 1) \n",
    "    return scaled + 1\n",
    "\n",
    "for each in dictonary.keys():\n",
    "    dictonary[each] = scale_100(dictonary[each])\n",
    "\n",
    "#sorted_dict = dict(sorted(dictonary.items(), key=lambda item: item[1], reverse=True))\n",
    "visited = []\n",
    "score_1 = 0\n",
    "unique = 0\n",
    "for word in corpus_list:\n",
    "    if word in dictonary.keys():\n",
    "        if word not in visited:\n",
    "            score_1 = score_1 + dictonary[word]\n",
    "            visited.append(word)\n",
    "            unique += 1\n",
    "        \n",
    "## calculating score_2 with tag ranks\n",
    "score_2 = 0\n",
    "db = pd.read_csv('dependencies/intro_Q1.csv')\n",
    "db_dict = dict(db.values)\n",
    "for word in corpus_list:\n",
    "    if word in db_dict.keys():\n",
    "        if word not in visited:\n",
    "            score_2 = score_2 + db_dict[word]\n",
    "            visited.append(word)\n",
    "            unique += 1\n",
    "\n",
    "strength = (alpha * score_1) + (beta * score_2)\n",
    "f.close()\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "#model = torch.load(\"models/bert-based-uncased.pth\")\n",
    "model.load_state_dict(torch.load('dependencies/bert-based-uncased-GED.pth'))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)\n",
    "#tokenizer = torch.load('models/bert-based-uncased.pth')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels =[0]\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "MAX_LEN = 128\n",
    "predictions = []\n",
    "true_labels = []\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(prediction_inputs.to(device).long(), token_type_ids=None, attention_mask=prediction_masks.to(device).long())\n",
    "logits = logits.detach().cpu().numpy()\n",
    "predictions.append(logits)\n",
    "\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "result = {}\n",
    "for i in range(len(flat_predictions)):\n",
    "    if flat_predictions[i] == 1:\n",
    "        result[sentences[i]] = 'Correct'\n",
    "    elif flat_predictions[i] == 0:\n",
    "        result[sentences[i]] = 'Wrong'\n",
    "\n",
    "        \n",
    "print('Captured text from audio: \\n', inp_corpus)\n",
    "print('Grammatical Syntax Analysis\\n', result)\n",
    "print(\"\\n Vocabulary strength: \", strength)\n",
    "print(\"\\n Unique words spoken: \", unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-bicycle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
